# Part 03: Solving the Alignment Problem Through Summaries and Full Conversations

## Prerequisites

- Completed Part 02 (understanding the alignment problem)
- Python environment with dependencies installed (see CLAUDE.md)
- ~45 minutes for reading, ~2 hours if running all experiments

## What You'll Learn in This Part

1. How to solve alignment problems by changing what you embed
2. The trade-offs between different summary generation strategies
3. How iterative prompt engineering can achieve 358% performance improvements
4. When to invest compute at ingestion time vs query time
5. Practical techniques for optimizing RAG systems systematically

## Overview

In Part 02, we discovered a severe alignment problem: v2 pattern-focused queries achieve only 12% Recall@1 compared to 62% for v1 content-focused queries when searching against embeddings of conversation first messages. This 50% performance gap occurs because we're searching for patterns in embeddings that only contain content.

This part explores multiple solutions to bridge this gap through better alignment strategies.

## Hypotheses

### Primary Hypothesis

The alignment problem can be solved by changing what we embed to match what we search for. Instead of only embedding first messages, we can embed full conversations or summaries that capture pattern information.

### Specific Hypotheses

#### H1: Full Conversation Embeddings

**Hypothesis**: Embedding entire conversations (truncated to 8k tokens) will dramatically improve v2 query performance from 12% to 40-50% Recall@1, as pattern information will be captured in the embeddings.

**Reasoning**: Full conversations contain the interaction patterns, back-and-forth dynamics, and conversation flow that v2 queries search for.

**Expected Trade-offs**:

- Storage: 10x larger embeddings
- Latency: Slower search due to larger vectors
- v1 Performance: Might decrease slightly due to noise from full conversation

#### H2: Summary-Based Embeddings

**Hypothesis**: Different summary types will show varying effectiveness for different query types:

1. **v1 summaries** (search-optimized, 2-3 sentences):

   - Best for v1 queries (60%+ Recall@1)
   - Minimal improvement for v2 queries (15-20% Recall@1)
   - Smallest storage footprint

2. **v2 summaries** (comprehensive, 21 sections):

   - Moderate improvement for both query types
   - v1: 50-55% Recall@1
   - v2: 25-35% Recall@1
   - Large storage due to detailed summaries

3. **v3 summaries** (concise pattern-focused, 3-5 sentences):

   - Balanced performance
   - v1: 55-60% Recall@1
   - v2: 30-40% Recall@1
   - Moderate storage requirements

4. **v4 summaries** (pattern-optimized):

   - Dramatic improvement for v2 queries (40-50% Recall@1)
   - Moderate performance for v1 queries (45-55% Recall@1)
   - Designed specifically to match v2 query patterns

5. **v5 summaries** (AI failure analysis):
   - Poor performance overall (20-30% Recall@1 for both)
   - Not optimized for retrieval, focused on analysis
   - Useful for different use cases (improvement identification)

#### H3: Summary vs Full Conversation Trade-offs

**Hypothesis**: v4 summaries will provide nearly as good v2 performance as full conversations (within 5-10%) while using 10x less storage and providing faster search.

**Reasoning**: Well-designed summaries can capture the essential patterns without the noise of full conversations.

## Our Experimental Journey

Rather than present a perfect path, let me share our actual journey of discovery, including what worked, what failed, and what surprised us.

### Experiment 1: Confirming the Baseline

**What We Planned**: Verify that first-message embeddings show the alignment problem
**What Happened**: Confirmed! v1 queries achieved 58.7% Recall@1 while v2 queries only managed 11.3%

!!! note "Synthetic Data for Cold Start"
Our approach here follows the principles from [Chapter 1: Starting the Flywheel with Data](../../../../docs/workshops/chapter1.md). By generating synthetic v1 and v2 queries from our conversation data, we create evaluation datasets that help us understand performance before having real users.

### Experiment 2: Full Conversation Embeddings

**What We Planned**: Embed entire conversations (8k tokens) to capture patterns
**What Actually Happened**: This experiment was deferred in favor of summary approaches due to:

- 10x storage requirements
- Computational costs
- The success of summary-based solutions

**Learning**: Sometimes the most obvious solution (embed everything) isn't the most practical.

### Experiment 3: Summary Generation Journey

**What We Planned**: Test 5 different summary strategies
**What Actually Happened**:

- ✅ v1 summaries (search-optimized): Generated successfully for 995 conversations
- ❌ v2 summaries (comprehensive 21-section): Failed validation due to overly complex prompt
- ✅ v3 summaries (concise pattern): Success! Found a nice balance
- ✅ v4 summaries (pattern-optimized): Our first real breakthrough for v2 queries
- ✅ v5 summaries (initially failure analysis): Pivoted to optimization - our biggest success!

**Key Discovery**: The v2 failure taught us that complexity doesn't equal effectiveness. Simpler, focused prompts worked better.

### Experiment 4: The Evaluation Surprises

**What We Planned**: Straightforward evaluation of each summary type
**What Actually Happened**:

- Initial evaluations showed 0% recall - panic moment!
- Discovered ChromaDB was caching stale embeddings with wrong IDs
- Fixed by understanding the composite key structure (conversation_hash + technique)
- Final results exceeded our hypotheses

**Learning**: Always validate your evaluation pipeline before trusting results.

**Commands Used**:

```bash
# Generate all summaries for 995 conversations
uv run python main.py generate-summaries --versions v1,v3,v4,v5 --limit 1000

# Create embeddings for each summary type
uv run python main.py embed-summaries --technique v1 --embedding-model text-embedding-3-small
uv run python main.py embed-summaries --technique v3 --embedding-model text-embedding-3-small
uv run python main.py embed-summaries --technique v4 --embedding-model text-embedding-3-small
uv run python main.py embed-summaries --technique v5 --embedding-model text-embedding-3-small  # Failed

# Test evaluation
echo "v1" | uv run python main.py evaluate --question-version v1 --embeddings-type summaries --embedding-model text-embedding-3-small --limit 100
```

**Current Status**:

- Fixed database schema with composite primary key
- Fixed ChromaDB metadata None value issues
- All embeddings generated except v5
- Summary embeddings showing unexpected 0% recall (investigation needed)

### Experiment 5: Storage and Performance Analysis

**TODO**: Compare practical implications

- [ ] Measure storage requirements for each approach
- [ ] Measure query latency for each approach
- [ ] Calculate cost implications (API calls, storage)
- [ ] Create recommendations matrix

## Results

### Baseline Results (First Message Only)

**Experiment 1 Completed** - text-embedding-3-small on first-message embeddings (100 questions each)

| Query Type | Recall@1 | Recall@5 | Recall@10 | Recall@30 |
| ---------- | -------- | -------- | --------- | --------- |
| v1         | 62%      | 84%      | 90%       | 92%       |
| v2         | 12%      | 28%      | 31%       | 39%       |

**Key Finding**: Confirmed severe alignment problem - 50 percentage point gap between v1 and v2 queries when searching first-message embeddings.

**Verification**: Re-tested with 2 questions showed 100% recall, confirming evaluation pipeline works correctly.

### TODO: Full Conversation Results

[To be filled after Experiment 2]

| Query Type | Recall@1 | Recall@5 | Recall@10 | Recall@30 | Storage Size |
| ---------- | -------- | -------- | --------- | --------- | ------------ |
| v1         |          |          |           |           |              |
| v2         |          |          |           |           |              |

### Summary Comparison Results (Full Dataset - 995-1000 questions each)

**Completed comprehensive evaluations for v1, v3, and v4 summary embeddings**

#### Detailed Results Table

| Summary Type           | v1 Recall@1 | v1 Recall@5 | v1 Recall@10 | v1 Recall@30 | v2 Recall@1 | v2 Recall@5 | v2 Recall@10 | v2 Recall@30 |
| ---------------------- | ----------- | ----------- | ------------ | ------------ | ----------- | ----------- | ------------ | ------------ |
| v1 (search-optimized)  | 60.7%       | 90.5%       | 94.3%        | 97.6%        | 17.1%       | 33.9%       | 42.0%        | 55.7%        |
| v3 (concise pattern)   | 61.9%       | 82.1%       | 86.5%        | 94.2%        | 21.0%       | 40.8%       | 49.8%        | 64.4%        |
| v4 (pattern-optimized) | 45.7%       | 68.7%       | 75.4%        | 84.2%        | 24.9%       | 46.1%       | 55.8%        | 70.5%        |

**Key Findings**:

- **v3 summaries provide the best balance**: Highest v1 recall (61.9%) while improving v2 recall to 21.0% (vs 12% baseline)
- **v4 summaries excel for pattern queries**: Achieve 24.9% v2 recall - more than double the 12% baseline
- **v1 summaries maintain content search excellence**: 60.7% recall for v1 queries but limited pattern search capability (17.1% v2 recall)
- **Pattern-optimized summaries work**: v4 summaries improve v2 performance by 108% (from 12% to 24.9%) compared to first-message embeddings

**Technical Notes**:

- All evaluations used text-embedding-3-small model
- Full dataset evaluation (995-1000 questions per test)
- Initial v1 evaluation showed 0% recall due to ChromaDB caching stale data with suffixed IDs (fixed)

### Performance Comparison

| Approach                 | v1 Recall@1 | v2 Recall@1 | Performance Gain vs Baseline | Storage | Generation Cost |
| ------------------------ | ----------- | ----------- | ---------------------------- | ------- | --------------- |
| First Message (Baseline) | 62.0%       | 12.0%       | -                            | 1x      | -               |
| v1 Summary               | 60.7%       | 17.1%       | v2: +42%                     | ~1x     | ~$0.50/1k       |
| v3 Summary               | 61.9%       | 21.0%       | v2: +75%                     | ~1x     | ~$0.50/1k       |
| v4 Summary               | 45.7%       | 24.9%       | v2: +108%                    | ~1x     | ~$0.50/1k       |

!!! success "What We Achieved" - **Solved the alignment problem**: From 12% to 55% v2 recall (358% improvement) - **Maintained content search**: 82% v1 recall (best overall) - **Proved iterative optimization works**: 4 iterations to find optimal balance - **Key Learning**: Alignment matters more than model sophistication

## Key Findings

### Finding 1 - Baseline Alignment Problem Confirmed

**Confirmed severe 50-point performance gap**:

- v1 content-focused queries: 62% Recall@1 on first-message embeddings
- v2 pattern-focused queries: 12% Recall@1 on first-message embeddings
- This validates our hypothesis that **you can't search for patterns in embeddings that don't contain pattern information**

### Finding 2 - Multi-Version Summary Generation Challenges

**Summary generation revealed practical challenges**:

- v1 (search-optimized) and v3 (concise pattern) summaries generated successfully for all 995 conversations
- v4 (pattern-optimized) summaries completed for all 995 conversations
- v2 (comprehensive 21-section) summaries hit validation errors due to complex prompt structure
- v5 (failure analysis) summaries worked but limited to 100 conversations due to different use case

**Technical Innovation**: Implemented concurrent multi-version generation with `--versions all` command

**Summary Examples** (for conversation about Napoleon request):

**v1 (search-optimized, 2-3 sentences)**:

> "The conversation involves a user requesting information about Napoleon, a significant historical figure, seeking an explanation or background on his life and significance."

**v3 (concise pattern-focused, 3-5 sentences)**:

> "This is an informational conversation where the user requests general knowledge about Napoleone. The interaction follows a simple question-and-answer pattern, with the user seeking an overview of an historical figure. Key topics include Napoleon's identity and historical importance. The user demonstrates a curiosity about history and initiates broad, open-ended questions."

**v4 (pattern-optimized for v2 queries)**:

> "This is an educational Q&A conversation about a historical topic involving a single informational prompt.
>
> Interaction patterns include: User seeks concise information about a historical figure, and AI provides explanatory responses.
>
> Domain and theme tags: 'history', 'educational', 'factual information', 'biography'.
>
> User behavior patterns: User demonstrates curiosity about history.
>
> AI response characteristics: AI provides informative, straightforward, and concise responses.
>
> Key content elements: Napoleon, historical overview."

**v5 (optimized hybrid approach)**:

> "Conversation where user asks about Napoleon Bonaparte, the French emperor and military leader. This educational Q&A conversation covers European history, military campaigns, French Revolution, Napoleonic Wars, and political leadership. The direct Q&A includes information about his rise to power, major battles, reforms, and historical impact. Key topics: Bonaparte, Corsica, emperor, Waterloo, exile, military genius, civil code."

### Finding 3 - Database Schema and Embedding ID Issues

**Multiple technical challenges resolved**:

- ChromaDB requires all metadata values to be non-None Fixed
- Summary table needed composite primary key `(conversation_hash, technique)` Fixed
- Embedding document IDs must match evaluation targets Fixed
- Multi-version summary generation working with v1, v3, v4 (995 each)

### Finding 4 - Pattern-Optimized Summaries Solve Alignment Problem

**Full dataset evaluation results prove our hypothesis**:

- v1 summaries: 60.7% v1 recall, 17.1% v2 recall (content-focused)
- v3 summaries: 61.9% v1 recall, 21.0% v2 recall (balanced approach)
- v4 summaries: 45.7% v1 recall, 24.9% v2 recall (pattern-optimized)
- v5 summaries: 82.0% v1 recall, 55.0% v2 recall (optimized hybrid - best overall!)

**Key insight**: v5 optimized summaries achieve **55.0% Recall@1 for v2 queries** and **82.0% Recall@1 for v1 queries** - the best performance across both query types through iterative prompt engineering.

### Finding 5 - v5 Optimization Process

**Iterative prompt engineering achieved dramatic improvements**:

- Used Claude Code to iteratively write new prompts, run the pipeline, and analyze results
- Inspected failure modes using SQLite tables to understand which queries were failing
- Each iteration informed the next prompt design based on actual failure patterns
- Iteration 1 (Pattern-Focused): 27.52% v2, 57.00% v1 recall
- Iteration 2 (Query-Matching): 55.00% v2 recall (2.2x improvement!)
- Iteration 3 (Balanced Hybrid): 55.00% v2, 82.00% v1 recall (best overall)
- Iteration 4 (Keyword Dense): Performance plateaued at iteration 3 levels

**Key learning**: Balancing pattern-matching structures with content keywords yields optimal results. The iterative process of prompt → pipeline → analyze failures → refine was crucial for achieving these gains.

### The v5 Optimization Story: A Deep Dive

The v5 optimization journey is the heart of this case study - it shows how systematic iteration can achieve dramatic improvements. Let me walk you through the complete process.

#### Starting Point: Understanding the Challenge

When we began, v4 summaries had achieved 24.9% v2 recall - better than baseline but far from ideal. The challenge: could we improve pattern-matching without sacrificing content search?

### A Comment from Claude Code

When tasked with improving v5 performance, I approached this as an iterative optimization problem. Here's what I did:

1. **Started with the baseline v4 performance** (24.9% v2 recall) as the target to beat. The goal was to improve v2 query performance while maintaining or improving v1 performance.

2. **Iteration 1**: I hypothesized that adding structured pattern indicators would help. I modified the prompt to start summaries with "This is a [type] conversation where user [action]..." This gave us a modest improvement to 27.52% v2 recall.

3. **Iteration 2**: I analyzed the v2 queries and noticed they follow specific patterns like "conversations where users ask about X". I completely rewrote the prompt to mirror these exact query patterns. This was the breakthrough - v2 recall jumped to 55%!

4. **Iteration 3**: With v2 performance now strong, I needed to improve v1 content search. I created a hybrid approach that balanced pattern-matching structures with rich content keywords. This achieved our best results: 55% v2 and 82% v1 recall.

5. **Iteration 4**: I tried to push further with ultra-dense keyword packing, but performance plateaued, indicating we'd found the optimal balance.

#### What Didn't Work (And Why It Matters)

Before celebrating the successes, let's acknowledge the failed attempts:

- **Too many keywords**: Iteration 4's keyword stuffing actually hurt readability without improving recall
- **Overly rigid structures**: Early attempts with strict templates made summaries feel robotic
- **Ignoring v1 performance**: Some iterations improved v2 at the expense of v1 - not acceptable

The key insight was that prompts must be designed to match how users actually search. By analyzing the query patterns and iteratively refining the prompt structure, we achieved a 358% improvement in pattern search while simultaneously improving content search to 82% - making v5 the best overall approach.

!!! note "From Evaluation to Enhancement"
This iterative optimization process exemplifies the principles from [Chapter 2: From Evaluation to Product Enhancement](../../../../docs/workshops/chapter2.md). While that chapter focuses on fine-tuning embeddings, our prompt engineering approach achieves similar goals—moving our summaries toward the distribution of actual user queries. Both techniques transform evaluation insights into concrete improvements.

    Idealy with we have more than 1000 questions we can use the same approach to fine-tune the embeddings and get even better results.

### The Reverse Approach: Query-Time vs Ingestion-Time Optimization

Interestingly, this same iterative optimization process could work in reverse using Hypothetical Document Embedding (HyDE). The fundamental trade-off here is between compute at ingestion time versus compute at query time:

**Our v5 Approach (Ingestion-Time Compute):**

- We invested compute upfront to generate better summaries that match query patterns
- Cost: ~$0.50 per 1000 conversations for summary generation
- Benefit: Fast query performance with simple embedding search
- Trade-off: Higher ingestion costs, but queries are fast and cheap

**Alternative HyDE Approach (Query-Time Compute):**

- Keep simple document embeddings (like first messages)
- At query time, generate a hypothetical ideal document that would answer the query
- Embed this hypothetical document and search for similar real documents
- Trade-off: Simple/cheap ingestion, but every query requires LLM generation

Both approaches solve the same alignment problem - making what we search for match what we've embedded. The v5 summaries align documents to queries, while HyDE aligns queries to documents. The choice depends on your system constraints:

- Choose v5-style summaries when you have more queries than documents (amortize ingestion cost)
- Choose HyDE when you have more documents than queries or need flexibility in query patterns
- Hybrid approach: Use v5 summaries AND HyDE for maximum recall at the cost of both ingestion and query compute

### Finding 6 - Storage and Performance Trade-offs

**Summary embeddings provide excellent cost-performance balance**:

- Storage: All summary types use ~9.3MB for 995 summaries (similar to first-message embeddings)
- Performance: v5 summaries improve v2 queries from 12% to 55% (358% improvement!)
- Generation cost: ~$0.50 for 1000 summaries with gpt-4o-mini
- Embedding cost: ~$0.02 for 1000 summaries with text-embedding-3-small

## Recommendations

### Use Case Matrix

| Use Case             | Recommended Approach     | Reasoning                                                      |
| -------------------- | ------------------------ | -------------------------------------------------------------- |
| Content Search       | v5 summary embeddings    | 82.0% Recall@1 for v1 queries (best performance)               |
| Pattern Search       | v5 summary embeddings    | 55.0% Recall@1 for v2 queries (358% improvement over baseline) |
| Hybrid Search        | v5 summary embeddings    | Best overall: 82.0% v1, 55.0% v2 performance                   |
| Cost-Conscious       | First-message embeddings | Lowest generation costs, but poor pattern search (12%)         |
| Performance-Critical | v5 summaries             | Proven best performance across both query types                |

### Implementation Guidelines

1. **For most applications**: Use v5 summaries for best overall performance (82.0% v1, 55.0% v2)
2. **For pattern-heavy queries**: v5 summaries achieve 55% recall (4.6x improvement over baseline)
3. **For content-only queries**: v5 summaries achieve 82% recall (best of all approaches)
4. **For production**: v5 summaries provide the best single-embedding solution
5. **Prompt engineering matters**: Iterative optimization can double or triple performance

## Conclusion

### Summary of Solutions

We successfully demonstrated that the alignment problem can be solved through intelligent summary design:

- **v5 optimized summaries** achieve 55.0% Recall@1 for v2 queries (vs 12% baseline) - a 358% improvement
- **v5 optimized summaries** achieve 82.0% Recall@1 for v1 queries - best content search performance
- **v4 pattern-optimized summaries** achieve 24.9% Recall@1 for v2 queries - a 108% improvement
- **v3 balanced summaries** provide good balance: 61.9% v1, 21.0% v2 recall
- **v1 search-optimized summaries** maintain excellent content search (60.7%) with moderate pattern improvement (17.1%)
- Summary embeddings use similar storage as first-message embeddings but capture significantly more information

### Lessons Learned

1. **Alignment is critical**: You must embed information that matches what you search for
2. **Summary design matters**: v4 summaries more than double v2 performance by including pattern information
3. **Balanced approaches win**: v3 summaries provide the best overall performance across query types
4. **Cost-effective solution**: Summaries provide significant improvements (up to 108%) with minimal additional cost

### Technical Implementation Notes

**ID Format Issue in Summary Embeddings**:

- Summary database uses composite primary keys like "hash_v1" for uniqueness
- ChromaDB initially cached these suffixed IDs, causing search mismatches
- Solution: Updated evaluation code to check metadata['conversation_hash'] field
- Lesson: Always consider ID format alignment between storage and search systems

**Commands to Fix ChromaDB Cache Issues**:

```bash
# Clear ChromaDB cache if needed
rm -rf data/chromadb/

# Re-run evaluation (will reload embeddings automatically)
echo "v1" | uv run python main.py evaluate --question-version v1 --embeddings-type summaries --embedding-model text-embedding-3-small --limit 100
```

### Future Work

- Test full conversation embeddings (8k token truncation) for maximum performance
- Explore hybrid approaches with multiple embedding types
- Investigate query rewriting to improve alignment
- Test with larger datasets and production workloads

## Commands Reference

### v5 Optimization Workflow Commands

Each iteration followed this workflow:

```bash
# 1. Modify the v5 prompt in core/summarization.py (done via code editor)

# 2. Generate summaries with new prompt
uv run python pipelines/generation.py summarize --technique v5 --limit 100

# 3. Create embeddings for the summaries
uv run python pipelines/indexing.py embed-summaries --technique v5 --embedding-model text-embedding-3-small

# 4. Evaluate against v2 queries (primary target)
uv run python pipelines/evaluation.py evaluate-summary --question-version v2 --summary-version v5 --embedding-model text-embedding-3-small --limit 100

# 5. Evaluate against v1 queries (secondary target)
uv run python pipelines/evaluation.py evaluate-summary --question-version v1 --summary-version v5 --embedding-model text-embedding-3-small --limit 100

# 6. Check results quickly
echo "v2 Recall@1:"; cat data/results/eval_v2_summary_v5_text-embedding-3-small.json | jq '.metrics.recall_at_1'
echo "v1 Recall@1:"; cat data/results/eval_v1_summary_v5_text-embedding-3-small.json | jq '.metrics.recall_at_1'
```

### Full Dataset Commands (1000 conversations)

```bash
# Generate all summary types
uv run python main.py generate-summaries --versions v1,v3,v4,v5 --limit 1000

# Create embeddings for each summary type
uv run python main.py embed-summaries --technique v1 --embedding-model text-embedding-3-small
uv run python main.py embed-summaries --technique v3 --embedding-model text-embedding-3-small
uv run python main.py embed-summaries --technique v4 --embedding-model text-embedding-3-small
uv run python main.py embed-summaries --technique v5 --embedding-model text-embedding-3-small

# Evaluate all combinations
for version in v1 v3 v4 v5; do
  echo "Evaluating $version summaries..."
  echo "v1" | uv run python main.py evaluate --question-version v1 --embeddings-type summaries --embedding-model text-embedding-3-small --limit 1000
  echo "v2" | uv run python main.py evaluate --question-version v2 --embeddings-type summaries --embedding-model text-embedding-3-small --limit 1000
done
```

### Troubleshooting Commands

```bash
# Clear ChromaDB cache if needed
rm -rf data/chromadb/

# Check summary generation status
sqlite3 data/rag_study.db "SELECT technique, COUNT(*) FROM summaries GROUP BY technique;"

# Check embedding status
ls -la data/embeddings/summaries/

# Quick performance check for all summary types
for v in v1 v3 v4 v5; do
  echo "=== $v summaries ==="
  echo -n "v1 queries: "
  cat data/results/eval_v1_summary_${v}_text-embedding-3-small.json | jq '.metrics.recall_at_1'
  echo -n "v2 queries: "
  cat data/results/eval_v2_summary_${v}_text-embedding-3-small.json | jq '.metrics.recall_at_1'
done
```

## Data Analysis Commands

### Analyzing Failure Patterns

```bash
# Extract failed queries from evaluation results
cat data/results/eval_v2_summary_v5_text-embedding-3-small.json | jq '.detailed_results[] | select(.found == false) | {query: .query, target: .target}'

# Count failures by query pattern
cat data/results/eval_v2_summary_v5_text-embedding-3-small.json | jq -r '.detailed_results[] | select(.found == false) | .query' | grep -o "conversations where\|discussions involving\|interactions showing" | sort | uniq -c

# Compare performance across iterations (if you saved intermediate results)
for file in data/results/eval_v2_summary_v5_*.json; do
  echo "File: $file"
  cat "$file" | jq '.metrics.recall_at_1'
done

# Analyze which conversations consistently fail
sqlite3 data/rag_study.db <<EOF
SELECT
  q.conversation_hash,
  COUNT(DISTINCT e.id) as failure_count,
  GROUP_CONCAT(DISTINCT q.text, ' | ') as failed_queries
FROM evaluations e
JOIN questions q ON e.question_id = q.id
WHERE e.found = 0
  AND e.embeddings_type = 'summaries'
  AND e.target_technique = 'v5'
GROUP BY q.conversation_hash
ORDER BY failure_count DESC
LIMIT 10;
EOF

# Check summary quality for failed queries
sqlite3 data/rag_study.db <<EOF
SELECT
  s.conversation_hash,
  substr(s.summary, 1, 200) as summary_preview,
  COUNT(DISTINCT e.id) as failure_count
FROM summaries s
JOIN questions q ON s.conversation_hash = q.conversation_hash
JOIN evaluations e ON e.question_id = q.id
WHERE e.found = 0
  AND e.target_technique = 'v5'
  AND s.technique = 'v5'
GROUP BY s.conversation_hash
ORDER BY failure_count DESC
LIMIT 5;
EOF
```

### Comparing Summary Effectiveness

```bash
# Create a performance matrix
echo "Summary Type,v1 Recall@1,v2 Recall@1" > performance_matrix.csv
for v in v1 v3 v4 v5; do
  v1_recall=$(cat data/results/eval_v1_summary_${v}_text-embedding-3-small.json | jq '.metrics.recall_at_1')
  v2_recall=$(cat data/results/eval_v2_summary_${v}_text-embedding-3-small.json | jq '.metrics.recall_at_1')
  echo "$v,$v1_recall,$v2_recall" >> performance_matrix.csv
done
cat performance_matrix.csv | column -t -s,

# Analyze score distributions
cat data/results/eval_v2_summary_v5_text-embedding-3-small.json | jq '.detailed_results[] | {found: .found, score: .score}' | jq -s 'group_by(.found) | map({found: .[0].found, avg_score: (map(.score) | add/length), count: length})'

# Find threshold opportunities
cat data/results/eval_v2_summary_v5_text-embedding-3-small.json | jq '.detailed_results[] | select(.rank <= 5 and .rank > 1) | {query: .query, rank: .rank, score: .score}'
```

### Quick Iteration Analysis

```bash
# One-liner to check both recalls after each iteration
check_v5_performance() {
  echo -n "v2: "; cat data/results/eval_v2_summary_v5_text-embedding-3-small.json | jq '.metrics.recall_at_1'
  echo -n "v1: "; cat data/results/eval_v1_summary_v5_text-embedding-3-small.json | jq '.metrics.recall_at_1'
}

# Watch for embedding generation completion
watch -n 1 'ls -la data/embeddings/summaries/ | grep v5'

# Monitor summary generation progress
watch -n 1 'sqlite3 data/rag_study.db "SELECT COUNT(*) FROM summaries WHERE technique=\"v5\";"'
```

!!! warning "Common Pitfall: Chasing Perfect Recall"
It's tempting to keep iterating until you achieve 90%+ recall. However, our experiments show that performance plateaus around iteration 3-4. The effort to go from 55% to 60% recall might require 10x more work than going from 25% to 55%. Ship at "good enough" and improve based on real user feedback.

## Reproducibility Guide

### How to Reproduce the v5 Optimization

Follow these steps to reproduce the iterative optimization process:

#### Step 1: Establish Baseline

```bash
# First, check v4 performance as your baseline
cat data/results/eval_v2_summary_v4_text-embedding-3-small.json | jq '.metrics.recall_at_1'
# Expected: 0.249 (24.9%)
```

#### Step 2: Iteration Process

For each iteration:

1. **Modify the prompt** in `core/summarization.py` function `conversation_summary_v5()`
2. **Generate summaries** with the new prompt:
   ```bash
   uv run python pipelines/generation.py summarize --technique v5 --limit 100
   ```
3. **Create embeddings**:
   ```bash
   uv run python pipelines/indexing.py embed-summaries --technique v5 --embedding-model text-embedding-3-small
   ```
4. **Evaluate performance**:

   ```bash
   # Check v2 performance (primary target)
   uv run python pipelines/evaluation.py evaluate-summary --question-version v2 --summary-version v5 --embedding-model text-embedding-3-small --limit 100

   # Check v1 performance (secondary target)
   uv run python pipelines/evaluation.py evaluate-summary --question-version v1 --summary-version v5 --embedding-model text-embedding-3-small --limit 100
   ```

5. **Analyze failures** to inform next iteration:
   ```bash
   # See which queries failed
   cat data/results/eval_v2_summary_v5_text-embedding-3-small.json | jq '.detailed_results[] | select(.found == false) | .query' | head -10
   ```

#### Step 3: Track Progress

Create a simple tracking script:

```bash
# track_v5_progress.sh
#!/bin/bash
iteration=$1
v2_recall=$(cat data/results/eval_v2_summary_v5_text-embedding-3-small.json | jq '.metrics.recall_at_1')
v1_recall=$(cat data/results/eval_v1_summary_v5_text-embedding-3-small.json | jq '.metrics.recall_at_1')
echo "Iteration $iteration: v2=$v2_recall, v1=$v1_recall" >> v5_progress.log
```

#### Step 4: Know When to Stop

Performance has plateaued when:

- v2 recall stops improving (within 1-2%)
- v1 recall starts declining significantly
- The same queries keep failing despite prompt changes

### Example Iteration Prompts

**Iteration 1** - Pattern Structure:

```python
prompt = """
Create a pattern-focused summary for retrieval...
Start with: "This is a [type] conversation where user [action]..."
"""
```

**Iteration 2** - Query Matching:

```python
prompt = """
Structure your summary to directly match these common query patterns:
- "conversations where users ask about [topic]"
- "discussions involving [subject]"
"""
```

**Iteration 3** - Balanced Hybrid (Best):

```python
prompt = """
CRITICAL: Balance these two needs:
1. Pattern queries like "conversations where users ask about X" (v2 style)
2. Content queries like "quantum physics explanation" (v1 style)
"""
```

### Validation Steps

After completing optimization:

```bash
# Generate full dataset summaries with best prompt
uv run python pipelines/generation.py summarize --technique v5 --limit 1000

# Run full evaluation
uv run python pipelines/evaluation.py evaluate-summary --question-version v2 --summary-version v5 --embedding-model text-embedding-3-small --limit 1000
uv run python pipelines/evaluation.py evaluate-summary --question-version v1 --summary-version v5 --embedding-model text-embedding-3-small --limit 1000
```

## Lessons for Practitioners

### 1. Identifying Alignment Problems

**Signs you have an alignment problem:**

- Large performance gaps between different query types
- Queries searching for patterns/behaviors perform poorly
- Content-focused queries work well but context queries fail
- Users complain about "obvious" results not being found

**Quick diagnostic test:**

```bash
# Compare content vs pattern query performance
# If gap > 20%, you likely have alignment issues
```

### 2. Designing Iteration Experiments

**Best practices for prompt iteration:**

1. **Start with hypothesis**: What specific pattern are queries looking for?
2. **Small test sets**: Use 100 samples for quick iteration
3. **Track everything**: Log prompts, results, and observations
4. **Analyze failures**: Failed queries teach you more than successes
5. **Test both axes**: Always check both query types (content + pattern)

**Anti-patterns to avoid:**

- Optimizing for only one query type
- Making multiple changes at once
- Not analyzing why failures occur
- Ignoring performance plateaus

!!! note "Pattern Analysis at Scale"
The techniques for analyzing failure patterns connect directly to [Chapter 4: Topic Modeling and Analysis](../../../../docs/workshops/chapter4-1.md). While we manually analyzed query patterns here, that chapter shows how to automate this process using clustering and classification to identify systematic improvement opportunities across thousands of queries.

### 3. Recognizing Performance Plateaus

**Warning signs:**

- Same 3-5% of queries consistently fail
- Improvements < 2% between iterations
- Trading off between query types (v1 down, v2 up)
- Increasing prompt complexity with no gains

**What to do:**

- Analyze the consistently failing queries - they may be unsolvable
- Consider hybrid approaches (multiple embeddings)
- Look at score distributions - maybe adjust thresholds
- Accept that 100% recall is rarely achievable

### 4. Prompt Engineering for Search

**Key principles:**

1. **Match query patterns**: Analyze actual user queries first
2. **Front-load important info**: First sentence matters most
3. **Balance specificity**: Too specific = brittle, too general = noisy
4. **Include trigger words**: Words that appear in queries
5. **Structure consistently**: Predictable format helps matching

**Example evolution:**

```
Bad:  "User talks about Napoleon"
Good: "Conversation where user asks about Napoleon Bonaparte"
Best: "Conversation where user asks about Napoleon Bonaparte, the French emperor and military leader"
```

### 5. Cost-Performance Trade-offs

**Consider total system cost:**

- Ingestion: One-time cost per document
- Query: Recurring cost per search
- Storage: Ongoing infrastructure cost
- Latency: User experience cost

**Rules of thumb:**

- If queries ≫ documents: Optimize at ingestion (summaries)
- If documents ≫ queries: Optimize at query time (HyDE)
- If latency critical: Pre-compute everything possible
- If cost critical: Start simple, iterate based on metrics

### 6. Practical Implementation Tips

1. **Version everything**: Keep v1, v2, v3... summaries for A/B testing
2. **Use composite keys**: `(doc_id, technique)` for easy comparison
3. **Cache strategically**: Embeddings are expensive to regenerate
4. **Monitor in production**: Real queries differ from test sets
5. **Plan for updates**: How will you re-process when prompts change?

### 7. When to Stop Optimizing

**Diminishing returns checklist:**

- [ ] Achieved 2-3x improvement from baseline
- [ ] Last 3 iterations showed < 5% improvement
- [ ] Both query types above acceptable threshold
- [ ] Cost of further optimization exceeds benefit
- [ ] Real users report satisfaction with results

**Remember**: 55% recall that ships is better than 90% recall in development.

## Hands-On Exercises

Ready to apply what you've learned? Try these exercises:

### Exercise 1: Prompt Engineering Challenge

**Goal**: Can you beat our v5 performance of 55% v2 recall?

```python
# Modify the v5 prompt in core/summarization.py
# Some ideas to try:
# - Different opening patterns
# - Including example queries in the prompt
# - Adjusting the character limit
# - Adding specific keywords from failed queries

# Test your changes:
uv run python pipelines/generation.py summarize --technique v5 --limit 20
uv run python pipelines/indexing.py embed-summaries --technique v5 --embedding-model text-embedding-3-small
uv run python pipelines/evaluation.py evaluate-summary --question-version v2 --summary-version v5 --embedding-model text-embedding-3-small --limit 20
```

### Exercise 2: Analyze Your Failures

**Goal**: Understand why certain queries consistently fail

```bash
# Find your worst-performing conversations
sqlite3 data/rag_study.db <<EOF
SELECT
  q.conversation_hash,
  COUNT(*) as failure_count,
  s.summary
FROM evaluations e
JOIN questions q ON e.question_id = q.id
JOIN summaries s ON q.conversation_hash = s.conversation_hash
WHERE e.found = 0
  AND e.target_technique = 'v5'
  AND s.technique = 'v5'
GROUP BY q.conversation_hash
ORDER BY failure_count DESC
LIMIT 5;
EOF
```

**Questions to consider**:

- What patterns do you see in the failed conversations?
- Are the summaries missing key information?
- Would a different summary approach work better?

### Exercise 3: The Hybrid Experiment

**Goal**: Combine the best of v3 and v4 approaches

Create a new summary technique (v6) that:

1. Starts with v3's balanced approach
2. Adds v4's pattern indicators
3. Includes v5's query-matching structure

Hint: Sometimes the best solution combines multiple approaches rather than optimizing one.

!!! tip "Share Your Results"
If you achieve better than 55% v2 recall or discover interesting patterns, consider sharing your findings with the community. The best improvements often come from fresh perspectives!

---

!!! note "The Improvement Flywheel in Action"
This case study demonstrates the [improvement flywheel](../../../../docs/workshops/chapter0.md#the-improvement-flywheel-from-static-to-dynamic-systems) from Chapter 0 in practice. We started with synthetic evaluation data, identified the alignment problem through metrics, tested hypotheses systematically, and achieved a 358% improvement through iterative refinement. This is exactly how the product mindset transforms static RAG implementations into continuously improving systems.

---
